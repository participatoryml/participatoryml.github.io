title,speakers,timespan,type,abstract,video,video_type
Opening Remarks,Organizing committee,1:00 PM - 1:15 PM UTC,Live talk (Iivestream),,,
AI’s Contradiction: King’s Radical Revolution in Values,Tawana Petty,1:15 PM - 1:45 PM UTC,Live talk (livestream),"
Dr. King called for a radical revolution of values in 1967. He understood that if we did not ""begin the shift from a thing-oriented society to a person-oriented society,"" and prioritize people over machines, computers and profit motives, we would be unable to undo the harms of racism, extreme materialism, and militarism. If we were to take Dr. King's challenge seriously today, how might we deepen our questions, intervene in harmful technologies and slow down innovation for innovation's sake?",https://slideslive.com/38931305/ais-contradiction-kings-radical-revolution-in-values,SlidesLive
What does it mean for ML to be trustworthy?,Nicolas Papernot,1:45 PM - 2:15 PM UTC,Talk (livestream),"
The attack surface of machine learning is large: training data can be poisoned, predictions manipulated using adversarial examples, models exploited to reveal sensitive information contained in training data, etc. This is in large parts due to the absence of security and privacy considerations in the design of ML algorithms. Yet, adversaries have clear incentives to target these systems. Thus, there is a need to ensure that computer systems that rely on ML are trustworthy.

Fortunately, we are at a turning point where ML is still being adopted, which creates a rare opportunity to address the shortcomings of the technology before it is widely deployed. Designing secure ML requires that we have a solid understanding as to what we expect legitimate model behavior to look like. We structure our discussion around three directions, which we believe are likely to lead to significant progress.

The first encompasses a spectrum of approaches to verification and admission control, which is a prerequisite to enable fail-safe defaults in machine learning systems. The second seeks to design mechanisms for assembling reliable records of compromise that would help understand the degree to which vulnerabilities are exploited by adversaries, as well as favor psychological acceptability of machine learning applications. The third pursues formal frameworks for security and privacy in machine learning, which we argue should strive to align machine learning goals such as generalization with security and privacy desiderata like robustness or privacy. We illustrate these directions with recent work on model extraction, privacy-preserving ML and machine unlearning.",https://www.youtube.com/watch?v=UpGgIqLhaqo,Youtube
Turning the tables on Facebook: How we audit Facebook using their own marketing tools,Piotr Sapieżyński,2:15 PM - 2:45 PM UTC,Talk (livestream),"
Researchers and journalists have found many ways that advertisers can target—or exclude—particular groups of users seeing their ads on Facebook, comparatively little attention has been paid to the implications of the platform's ad delivery process, where the platform decides which users see which ads. In this talk I will show how we audit Facebook's delivery algorithms for potential gender and race discrimination using Facebook's own tools tools designed to assist advertisers. Following these methods we find that Facebook delivers different job ads to men and women as well as white and Black users, despite inclusive targeting. We also identify how Facebook contributes to creating opinion filter bubbles by steering political ads towards users who already agree with their content.",https://slideslive.com/38930900/,SlidesLive
Poster Session 1,,2:45 PM - 3:30 PM UTC,Poster session (Discord),,,
Breakout Sessions 1 / Break,,3:30 PM - 4:15 PM UTC,Breakout discussions (Discord),,,
Panel 1,"Tawana Petty, Nicolas Papernot, Piotr Sapieżyński, Aleksandra Korolova, Deborah Raji (moderator)",4:15 PM - 5:00 PM UTC,Panel (livestream),,https://slideslive.com/38932899/panel-1,
Affected Community Perspectives on Algorithmic Decision-Making in Child Welfare Services ,Alexandra Chouldechova,5:00 PM - 5:30 PM UTC,Talk (livestream),"
Algorithmic decision-making systems are increasingly being adopted by government public service agencies. Researchers, policy experts, and civil rights groups have all voiced concerns that such systems are being deployed without adequate consideration of potential harms, disparate impacts, and public accountability practices. Yet little is known about the concerns of those most likely to be affected by these systems. In this talk I will discuss what we learned from a series of workshops conducted to better understand the concerns of affected communities in the context of child welfare services. Through these workshops we learned about the perspectives of families involved in the child welfare system, employees of child welfare agencies, and service providers.",https://slideslive.com/38930606/,SlidesLive
Actionable Recourse in Machine Learning ,Berk Ustun,5:30 PM - 6:00 PM UTC,Talk (livestream),"Machine learning models are often used to automate decisions that affect consumers: whether to approve a loan, a credit card application or provide insurance. In such tasks, consumers should have the ability to change the decision of the model. When a consumer is denied a loan by a credit score, for example, they should be able to alter its input variables in a way that guarantees approval. Otherwise, they will be denied the loan so long as the model is deployed, and – more importantly – lack control over a decision that affects their livelihood. In this talk, I will formally discuss these issues in terms of a notion called recourse -- i.e., the ability of a person to change the decision of a model by altering actionable input variables. I will describe how machine learning models may fail to provide recourse due to standard practices in model development. I will then describe integer programming tools to verify recourse in linear classification models. I will end with a brief discussion on how recourse can facilitate meaningful consumer protection in modern applications of machine learning. This is joint work with Alexander Spangher and Yang Liu.",https://slideslive.com/38930604/,SlidesLive
Beyond Fairness and Ethics: Towards Agency and Shifting Power,Jamelle Watson-Daniels,6:00 PM - 6:30 PM UTC,Talk (livestream),"When we consider power imbalances between those who craft ML systems and those most vulnerable to the impacts of those systems, what is often enabling that power is the localization of control in the hands of tech companies and technical experts who consolidate power using claims to perceived scientific objectivity and legal protections of intellectual property. At the same time, there is a legacy in the scientific community of data being wielded as an instrument of oppression, often reinforcing inequality and perpetuating injustice. At Data for Black Lives, we bring together scientists and community-based activists to take collective action using data for fighting bias, building progressive movements, and promoting civic engagement. In the ML community, people often take for granted the initial steps in the process of crafting ML systems that involve data collection, storage and access. Researchers often engage with datasets as if they appeared spontaneously with no social context. One method of moving beyond fairness metrics and generic discussions of ethics to meaningfully shifting agency to the people most marginalized is to stop ignoring the context, construction and implications of the datasets we use for research. I offer two considerations for shifting power in this way: Intentional data narratives and Data trusts - an alternative to current strategies of data governance.",https://slideslive.com/38930952/,SlidesLive
Panel 2,"Berk Ustun, Alexandra Chouldechova, Jamelle Watson-Daniels, Deborah Raji (moderator)",6:30 PM - 7:15 PM UTC,Panel (livestream),,https://slideslive.com/38932900/panel-2,
Poster Session 2,,7:15 PM - 8:00 PM UTC,Poster session (Discord),,,
Breakout Sessions 2,,8:00 PM - 8:45 PM UTC,Breakout discussions (Discord),,,
